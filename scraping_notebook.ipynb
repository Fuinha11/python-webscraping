{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bitc2cac26cbf9d40b49627b81e24515145",
   "display_name": "Python 3.6.9 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Scraping with Python\n",
    "\n",
    "This is my notebook to learn how to web scrape with Python.\n",
    "\n",
    "So here we go!\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Lesson 1\n",
    "\n",
    "### Basic to web scraping:\n",
    "- Lybraries we will use:\n",
    "    - urllib\n",
    "    - BeautifilSoup (bs4)\n",
    "\n",
    "### How to Scrape:\n",
    "1. Use urllib to make a request to the web page you want.\n",
    "1. Parse the html to a BeautifilSoup object.\n",
    "1. Find the data you need in the object.\n",
    "1. You're done.\n",
    "        "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we have a simple exemple of the process:\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://alura-site-scraping.herokuapp.com/hello-world.php\"\n",
    "response = urlopen(url) # Get on url\n",
    "html = response.read() # Read the html data from response\n",
    "soup = BeautifulSoup(html, 'html.parser') # Parse HTML to a Beautiful Soup OBJ\n",
    "print(soup.find('h1', id='hello-world').get_text()) # Find the desired tag\n",
    "print(soup.find('p').get_text())"
   ]
  },
  {
   "source": [
    "## Lesson 2\n",
    "\n",
    "### Improving a request:\n",
    "Sometimes we need to improve a request to make it work with the page we need.\n",
    "This can be adding a authorization or other headers or even a request body. To do this we create a Request and add things to this request, as in the code bellow.\n",
    "\n",
    "### Cleaning up the data:\n",
    "\n",
    "The object we get from response.read() is of type bytes. It's sometimes better to convert it to a str as strs have easier and greater options for a treatment.\n",
    "\n",
    "It's always a good idea to decode and clean up the data we received to improve the convertion to a Beautiful Soup object.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from urllib.request import Request, urlopen\n",
    "\n",
    "url = 'https://www.alura.com.br'\n",
    "headers = {'User-Agent': 'Chrome/76.0.3809.100'} # Creating a headers dictionary.\n",
    "\n",
    "req = Request(url, headers = headers) # Creating the request using the headers dictionary.\n",
    "response = urlopen(req)\n",
    "b = response.read()\n",
    "\n",
    "html = b.decode('utf-8') # Decoding bytes to str using UTF-8.\n",
    "html = html.split() # Spliting the str in empty spaces, line breaks and tabs.\n",
    "html = \" \".join(html) # Re-joining the data with only 1 whitespace between everything.\n",
    "html = html.replace('> <', '><') # Removing the whitespace between tags.\n",
    "\n",
    "# For a better use let's create a function to do all this for us.\n",
    "\n",
    "def clean_input(input):\n",
    "    return \" \".join(input.split()).replace('> <', '><')\n"
   ]
  },
  {
   "source": [
    "## Lesson 3\n",
    "\n",
    "### Working with bs4 BeautifulSoup:\n",
    "- Parse a html string using the html.parser\n",
    "- To improve visualization use soup.prettify()\n",
    "- We can find tags using soup.find('tagname')\n",
    "- Once on a tag we can find it's atributes with tag.attrs()\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_input(input):\n",
    "    return \" \".join(input.split()).replace('> <', '><')\n",
    "\n",
    "url = 'https://www.alura.com.br'\n",
    "headers = {'User-Agent': 'Chrome/76.0.3809.100'} # Creating a headers dictionary.\n",
    "\n",
    "req = Request(url, headers = headers) # Creating the request using the headers dictionary.\n",
    "response = urlopen(req)\n",
    "b = response.read()\n",
    "\n",
    "html = clean_input(b.decode('utf-8')) # Decoding and cleaning.\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser') # Parsing.\n",
    "soup.prettify() # Prettify\n",
    "\n",
    "print(soup.find('h1')) # Find first h1\n",
    "print(soup.find('h1').attrs) # Get atributes from first h1"
   ]
  },
  {
   "source": [
    "## Lesson 4\n",
    "\n",
    "### Find() and find_all()\n",
    "\n",
    "You can use find() to find a single tag and find_all() for multiple tags acordingly to some filters.\n",
    "\n",
    "- find(tag, attributes, recursive, text, **kwargs)\n",
    "- find_all(tag, attributes, recursive, text, limit, **kwargs)\n",
    "\n",
    "### Find_parent() and find_siblings()\n",
    "\n",
    "Find parents and siblings are an easy way to navigate throught the html to get where you want to get\n",
    "\n",
    "- find_parent(tag, attributes, text, **kwargs)\n",
    "- find_parents(tag, attributes, text, limit **kwargs)\n",
    "- find_next_sibling(tag, attributes, text, **kwargs)\n",
    "- find_next_siblings(tag, attributes, text, limit **kwargs)\n",
    "- find_previous_sibling(tag, attributes, text, **kwargs)\n",
    "- find_previous_siblings(tag, attributes, text, limit **kwargs)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Lesson 5\n",
    "\n",
    "### Identifying and storing data.\n",
    "\n",
    "Now we need to identify where in the html the data we need is and store it.\n",
    "\n",
    "- Open the page in a web browser and analyse it.\n",
    "- Figure out where the important data is.\n",
    "- Store this data in a dictionary or a database.\n",
    "\n",
    "### Working with Data Frames\n",
    "\n",
    "Data Frames are usefull to convert your data to a file better suited for data analisys.\n",
    "\n",
    "- We will use Pandas\n",
    "- Create a dataframe from our dict and export it to a file.\n",
    "\n",
    "### Storing images\n",
    "\n",
    "Sometimes we are interested in storing images as well. To do this we use urlretrieve.\n",
    "\n",
    "- Find the source (src) of the image.\n",
    "- Use urlretrieve to download it to a directory.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from urllib.request import Request, urlopen, urlretrieve\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def clean_input(input):\n",
    "    return \" \".join(input.split()).replace('> <', '><')\n",
    "\n",
    "url = 'https://alura-site-scraping.herokuapp.com/index.php'\n",
    "headers = {'User-Agent': 'Chrome/76.0.3809.100'} # Creating a headers dictionary.\n",
    "\n",
    "req = Request(url, headers = headers) # Creating the request using the headers dictionary.\n",
    "response = urlopen(req)\n",
    "b = response.read()\n",
    "\n",
    "html = clean_input(b.decode('utf-8')) # Decoding and cleaning.\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser') # Parsing.\n",
    "\n",
    "cards = [] # Array to hold all cards.\n",
    "card = {} # Dictionary to store the information.\n",
    "\n",
    "anuncio = soup.find('div', {'class': 'well card'}) # Find one add in the page.\n",
    "\n",
    "infos = anuncio.find('div', {'class':'body-card'}).find_all('p') # Find the infos inside a body of card.\n",
    "\n",
    "for info in infos:\n",
    "    card[info.get('class')[0].split('-')[-1]] = info.get_text() # Storing infos to dict dynamicaly.\n",
    "\n",
    "card['valor'] = anuncio.find('p',{'class':'txt-value'}).get_text() # Storing more info from other divs\n",
    "\n",
    "items = anuncio.find('div', {'class':'body-card'}).ul.find_all('li') # Find the accessories inside a body of card.\n",
    "\n",
    "items.pop() # Removing last item '...'\n",
    "\n",
    "accessories = [] # Create array for accessories\n",
    "\n",
    "for a in items :\n",
    "        accessories.append(a.get_text().replace('â–º ', '')) # Adding acessories in array\n",
    "\n",
    "card['accessories'] = accessories # Adding to dict\n",
    "\n",
    "dataset = pd.DataFrame.from_dict(card, orient = 'index').T # Creating a data Frame from our card data.\n",
    "dataset.to_csv('./output/data/dataset.csv', sep=';', index = False, encoding = 'utf-8-sig') # exporting the info.\n",
    "\n",
    "image = anuncio.find('div', {'class':'image-card'}).find('img') # Find the image inside the card.\n",
    "urlretrieve(image.get('src'), './output/img/' + image.get('src').split('/')[-1]) # Downloading the image to a data folder"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "## Lesson 6\n",
    "\n",
    "### Scrapping the entire page\n",
    "\n",
    "Apply what we learned on the previous lesson to all adds on the page."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                     name category            motor           description  \\\n0   LAMBORGHINI AVENTADOR    USADO    Motor 1.8 16v  Ano 1993 - 55.286 km   \n1                  BMW M2    USADO    Motor 3.0 32v  Ano 2018 - 83.447 km   \n2                    ALFA    USADO    Motor 1.8 16v  Ano 2004 - 19.722 km   \n3                   PUECH    USADO  Motor Diesel V8  Ano 1992 - 34.335 km   \n4  LAMBORGHINI MURCIELAGO    USADO     Motor 1.0 8v     Ano 1991 - 464 km   \n5            ASTON MARTIN    USADO  Motor Diesel V6  Ano 2004 - 50.189 km   \n6                     TVR    USADO  Motor 4.0 Turbo  Ano 2014 - 17.778 km   \n7               EXCALIBUR    USADO    Motor 3.0 32v  Ano 2009 - 81.251 km   \n8                 MCLAREN     NOVO     Motor Diesel       Ano 2019 - 0 km   \n9                  TOYOTA    USADO  Motor 4.0 Turbo  Ano 1999 - 12.536 km   \n\n              location       valor  \\\n0  Belo Horizonte - MG  R$ 338.000   \n1  Belo Horizonte - MG  R$ 346.000   \n2  Rio de Janeiro - RJ  R$ 480.000   \n3       SÃ£o Paulo - SP  R$ 133.000   \n4  Belo Horizonte - MG  R$ 175.000   \n5  Belo Horizonte - MG  R$ 239.000   \n6  Belo Horizonte - MG  R$ 115.000   \n7  Rio de Janeiro - RJ  R$ 114.000   \n8       SÃ£o Paulo - SP   R$ 75.000   \n9       SÃ£o Paulo - SP  R$ 117.000   \n\n                                         accessories   opportunity  \n0  [4 X 4, CÃ¢mera de estacionamento, Controle de ...           NaN  \n1  [CÃ¢mera de estacionamento, Controle de estabil...           NaN  \n2  [Central multimÃ­dia, Bancos de couro, Rodas de...           NaN  \n3  [Bancos de couro, Freios ABS, Rodas de liga, C...           NaN  \n4  [Central multimÃ­dia, Teto panorÃ¢mico, Sensor c...           NaN  \n5  [Painel digital, Controle de traÃ§Ã£o, Teto pano...  OPORTUNIDADE  \n6  [4 X 4, Teto panorÃ¢mico, Central multimÃ­dia, C...           NaN  \n7  [Painel digital, CÃ¢mbio automÃ¡tico, Sensor de ...           NaN  \n8  [Central multimÃ­dia, CÃ¢mera de estacionamento,...           NaN  \n9  [Bancos de couro, Freios ABS, Piloto automÃ¡tic...  OPORTUNIDADE  \n"
     ]
    }
   ],
   "source": [
    "from urllib.request import Request, urlopen, urlretrieve\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def clean_input(input):\n",
    "    return \" \".join(input.split()).replace('> <', '><')\n",
    "\n",
    "def parse_anuncio(anuncio):\n",
    "    card = {} # Dictionary to store the information.\n",
    "\n",
    "    infos = anuncio.find('div', {'class':'body-card'}).find_all('p') # Find the infos inside a body of card.\n",
    "\n",
    "    for info in infos:\n",
    "        card[info.get('class')[0].split('-')[-1]] = info.get_text() # Storing infos to dict dynamicaly.\n",
    "\n",
    "    card['valor'] = anuncio.find('p',{'class':'txt-value'}).get_text() # Storing more info from other divs\n",
    "\n",
    "    items = anuncio.find('div', {'class':'body-card'}).ul.find_all('li') # Find the accessories inside a body of card.\n",
    "    items.pop() # Removing last item '...'\n",
    "    accessories = [] # Create array for accessories\n",
    "\n",
    "    for a in items :\n",
    "            accessories.append(a.get_text().replace('â–º ', '')) # Adding acessories in array\n",
    "    card['accessories'] = accessories # Adding to dict\n",
    "\n",
    "    image = anuncio.find('div', {'class':'image-card'}).find('img') # Find the image inside the card.\n",
    "    urlretrieve(image.get('src'), './output/img/' + image.get('src').split('/')[-1]) # Downloading the image to a data folder\n",
    "\n",
    "    return card\n",
    "\n",
    "def scrape_page():\n",
    "    url = 'https://alura-site-scraping.herokuapp.com/index.php'\n",
    "    headers = {'User-Agent': 'Chrome/76.0.3809.100'} # Creating a headers dictionary.\n",
    "    req = Request(url, headers = headers) # Creating the request using the headers dictionary.\n",
    "    response = urlopen(req)\n",
    "    b = response.read()\n",
    "    html = clean_input(b.decode('utf-8')) # Decoding and cleaning.\n",
    "    soup = BeautifulSoup(html, 'html.parser') # Parsing.\n",
    "\n",
    "    cards = [] # Array to hold all cards.\n",
    "\n",
    "    anuncios = soup.find('div', {\"id\": \"container-cards\"}).findAll('div', class_=\"card\") # Find all adds in page \n",
    "\n",
    "    for anuncio in anuncios: # Loop through adds\n",
    "        cards.append(parse_anuncio(anuncio)) # Parse and add to list.\n",
    "\n",
    "    dataset = pd.DataFrame(cards) # Creating a data Frame from our card data.\n",
    "    dataset.to_csv('./output/data/dataset.csv', sep=';', index = False, encoding = 'utf-8-sig') # exporting the info.\n",
    "\n",
    "    return dataset\n",
    "\n",
    "print(scrape_page())"
   ]
  },
  {
   "source": [
    "## Lesson 7\n",
    "\n",
    "### Scrapping the entire site.\n",
    "\n",
    "Apply what we learned previously to scrape the website and all it's pages."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                       name category            motor            description  \\\n0     LAMBORGHINI AVENTADOR    USADO    Motor 1.8 16v   Ano 1993 - 55.286 km   \n1                    BMW M2    USADO    Motor 3.0 32v   Ano 2018 - 83.447 km   \n2                      ALFA    USADO    Motor 1.8 16v   Ano 2004 - 19.722 km   \n3                     PUECH    USADO  Motor Diesel V8   Ano 1992 - 34.335 km   \n4    LAMBORGHINI MURCIELAGO    USADO     Motor 1.0 8v      Ano 1991 - 464 km   \n..                      ...      ...              ...                    ...   \n241           SUV REAR TIRE    USADO    Motor 3.0 32v   Ano 1998 - 74.292 km   \n242                 ANTIQUE     NOVO    Motor 2.0 16v        Ano 2019 - 0 km   \n243                   SPORT    USADO    Motor 2.0 16v  Ano 2001 - 102.776 km   \n244                IMPERIAL    USADO    Motor 1.8 16v  Ano 2011 - 101.787 km   \n245          KIA SPORTS CAR    USADO    Motor 3.0 32v   Ano 2001 - 88.564 km   \n\n                location       valor  \\\n0    Belo Horizonte - MG  R$ 338.000   \n1    Belo Horizonte - MG  R$ 346.000   \n2    Rio de Janeiro - RJ  R$ 480.000   \n3         SÃ£o Paulo - SP  R$ 133.000   \n4    Belo Horizonte - MG  R$ 175.000   \n..                   ...         ...   \n241       SÃ£o Paulo - SP  R$ 489.000   \n242  Belo Horizonte - MG  R$ 427.000   \n243  Belo Horizonte - MG  R$ 203.000   \n244  Belo Horizonte - MG  R$ 474.000   \n245  Belo Horizonte - MG  R$ 366.000   \n\n                                           accessories   opportunity  \n0    [4 X 4, CÃ¢mera de estacionamento, Controle de ...           NaN  \n1    [CÃ¢mera de estacionamento, Controle de estabil...           NaN  \n2    [Central multimÃ­dia, Bancos de couro, Rodas de...           NaN  \n3    [Bancos de couro, Freios ABS, Rodas de liga, C...           NaN  \n4    [Central multimÃ­dia, Teto panorÃ¢mico, Sensor c...           NaN  \n..                                                 ...           ...  \n241  [CÃ¢mera de estacionamento, Rodas de liga, Sens...           NaN  \n242  [Bancos de couro, Freios ABS, Sensor de estaci...           NaN  \n243  [Sensor crepuscular, Sensor de chuva, Vidros e...           NaN  \n244  [Painel digital, Travas elÃ©tricas, Sensor de c...  OPORTUNIDADE  \n245  [Sensor crepuscular, Bancos de couro, Sensor d...           NaN  \n\n[246 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import Request, urlopen, urlretrieve\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def clean_input(input):\n",
    "    return \" \".join(input.split()).replace('> <', '><')\n",
    "\n",
    "def parse_anuncio(anuncio):\n",
    "    card = {} # Dictionary to store the information.\n",
    "\n",
    "    infos = anuncio.find('div', {'class':'body-card'}).find_all('p') # Find the infos inside a body of card.\n",
    "\n",
    "    for info in infos:\n",
    "        card[info.get('class')[0].split('-')[-1]] = info.get_text() # Storing infos to dict dynamicaly.\n",
    "\n",
    "    card['valor'] = anuncio.find('p',{'class':'txt-value'}).get_text() # Storing more info from other divs\n",
    "\n",
    "    items = anuncio.find('div', {'class':'body-card'}).ul.find_all('li') # Find the accessories inside a body of card.\n",
    "    items.pop() # Removing last item '...'\n",
    "    accessories = [] # Create array for accessories\n",
    "\n",
    "    for a in items :\n",
    "            accessories.append(a.get_text().replace('â–º ', '')) # Adding acessories in array\n",
    "    card['accessories'] = accessories # Adding to dict\n",
    "\n",
    "    # image = anuncio.find('div', {'class':'image-card'}).find('img') # Find the image inside the card.\n",
    "    # urlretrieve(image.get('src'), './output/img/' + image.get('src').split('/')[-1]) # Downloading the image to a data folder\n",
    "\n",
    "    return card\n",
    "\n",
    "def scrape_page(page=1):\n",
    "    url = 'https://alura-site-scraping.herokuapp.com/index.php?page=' + str(page)\n",
    "    headers = {'User-Agent': 'Chrome/76.0.3809.100'} # Creating a headers dictionary.\n",
    "    req = Request(url, headers = headers) # Creating the request using the headers dictionary.\n",
    "    response = urlopen(req)\n",
    "    b = response.read()\n",
    "    html = clean_input(b.decode('utf-8')) # Decoding and cleaning.\n",
    "    soup = BeautifulSoup(html, 'html.parser') # Parsing.\n",
    "\n",
    "    cards = [] # Array to hold all cards.\n",
    "\n",
    "    anuncios = soup.find('div', {\"id\": \"container-cards\"}).findAll('div', class_=\"card\") # Find all adds in page \n",
    "\n",
    "    for anuncio in anuncios: # Loop through adds\n",
    "        cards.append(parse_anuncio(anuncio)) # Parse and add to list.\n",
    "\n",
    "    return cards\n",
    "\n",
    "def scrape_site():\n",
    "    url = 'https://alura-site-scraping.herokuapp.com/index.php'\n",
    "    headers = {'User-Agent': 'Chrome/76.0.3809.100'} # Creating a headers dictionary.\n",
    "    req = Request(url, headers = headers) # Creating the request using the headers dictionary.\n",
    "    response = urlopen(req)\n",
    "    b = response.read()\n",
    "    html = clean_input(b.decode('utf-8')) # Decoding and cleaning.\n",
    "    soup = BeautifulSoup(html, 'html.parser') # Parsing.\n",
    "    total_pages = int(soup.find('span', class_='info-pages').get_text().split()[-1]) # Find total pages for search\n",
    "\n",
    "    all_cards = []\n",
    "    for page in range(1,total_pages+1):\n",
    "        all_cards.extend(scrape_page(page))\n",
    "\n",
    "    dataset = pd.DataFrame(all_cards) # Creating a data Frame from our card data.\n",
    "    dataset.to_csv('./output/data/dataset.csv', sep=';', index = False, encoding = 'utf-8-sig') # exporting the info.\n",
    "\n",
    "    return dataset\n",
    "\n",
    "print(scrape_site())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}