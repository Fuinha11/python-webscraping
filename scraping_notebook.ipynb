{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bitc2cac26cbf9d40b49627b81e24515145",
   "display_name": "Python 3.6.9 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Scraping with Python\n",
    "\n",
    "This is my notebook to learn how to web scrape with Python.\n",
    "\n",
    "So here we go!\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Lesson 1\n",
    "\n",
    "### Basic to web scraping:\n",
    "- Lybraries we will use:\n",
    "    - urllib\n",
    "    - BeautifilSoup (bs4)\n",
    "\n",
    "### How to Scrape:\n",
    "1. Use urllib to make a request to the web page you want.\n",
    "1. Parse the html to a BeautifilSoup object.\n",
    "1. Find the data you need in the object.\n",
    "1. You're done.\n",
    "        "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we have a simple exemple of the process:\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://alura-site-scraping.herokuapp.com/hello-world.php\"\n",
    "response = urlopen(url) # Get on url\n",
    "html = response.read() # Read the html data from response\n",
    "soup = BeautifulSoup(html, 'html.parser') # Parse HTML to a Beautiful Soup OBJ\n",
    "print(soup.find('h1', id='hello-world').get_text()) # Find the desired tag\n",
    "print(soup.find('p').get_text())"
   ]
  },
  {
   "source": [
    "## Lesson 2\n",
    "\n",
    "### Improving a request:\n",
    "Sometimes we need to improve a request to make it work with the page we need.\n",
    "This can be adding a authorization or other headers or even a request body. To do this we create a Request and add things to this request, as in the code bellow.\n",
    "\n",
    "### Cleaning up the data:\n",
    "\n",
    "The object we get from response.read() is of type bytes. It's sometimes better to convert it to a str as strs have easier and greater options for a treatment.\n",
    "\n",
    "It's always a good idea to decode and clean up the data we received to improve the convertion to a Beautiful Soup object.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from urllib.request import Request, urlopen\n",
    "\n",
    "url = 'https://www.alura.com.br'\n",
    "headers = {'User-Agent': 'Chrome/76.0.3809.100'} # Creating a headers dictionary.\n",
    "\n",
    "req = Request(url, headers = headers) # Creating the request using the headers dictionary.\n",
    "response = urlopen(req)\n",
    "b = response.read()\n",
    "\n",
    "html = b.decode('utf-8') # Decoding bytes to str using UTF-8.\n",
    "html = html.split() # Spliting the str in empty spaces, line breaks and tabs.\n",
    "html = \" \".join(html) # Re-joining the data with only 1 whitespace between everything.\n",
    "html = html.replace('> <', '><') # Removing the whitespace between tags.\n",
    "\n",
    "# For a better use let's create a function to do all this for us.\n",
    "\n",
    "def clean_input(input):\n",
    "    return \" \".join(input.split()).replace('> <', '><')\n"
   ]
  },
  {
   "source": [
    "## Lesson 3\n",
    "\n",
    "### Working with bs4 BeautifulSoup:\n",
    "- Parse a html string using the html.parser\n",
    "- To improve visualization use soup.prettify()\n",
    "- We can find tags using soup.find('tagname')\n",
    "- Once on a tag we can find it's atributes with tag.attrs()\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_input(input):\n",
    "    return \" \".join(input.split()).replace('> <', '><')\n",
    "\n",
    "url = 'https://www.alura.com.br'\n",
    "headers = {'User-Agent': 'Chrome/76.0.3809.100'} # Creating a headers dictionary.\n",
    "\n",
    "req = Request(url, headers = headers) # Creating the request using the headers dictionary.\n",
    "response = urlopen(req)\n",
    "b = response.read()\n",
    "\n",
    "html = clean_input(b.decode('utf-8')) # Decoding and cleaning.\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser') # Parsing.\n",
    "soup.prettify() # Prettify\n",
    "\n",
    "print(soup.find('h1')) # Find first h1\n",
    "print(soup.find('h1').attrs) # Get atributes from first h1"
   ]
  },
  {
   "source": [
    "## Lesson 4\n",
    "\n",
    "### Find() and find_all()\n",
    "\n",
    "You can use find() to find a single tag and find_all() for multiple tags acordingly to some filters.\n",
    "\n",
    "- find(tag, attributes, recursive, text, **kwargs)\n",
    "- find_all(tag, attributes, recursive, text, limit, **kwargs)\n",
    "\n",
    "### Find_parent() and find_siblings()\n",
    "\n",
    "Find parents and siblings are an easy way to navigate throught the html to get where you want to get\n",
    "\n",
    "- find_parent(tag, attributes, text, **kwargs)\n",
    "- find_parents(tag, attributes, text, limit **kwargs)\n",
    "- find_next_sibling(tag, attributes, text, **kwargs)\n",
    "- find_next_siblings(tag, attributes, text, limit **kwargs)\n",
    "- find_previous_sibling(tag, attributes, text, **kwargs)\n",
    "- find_previous_siblings(tag, attributes, text, limit **kwargs)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Lesson 5\n",
    "\n",
    "### Identifying and storing data.\n",
    "\n",
    "Now we need to identify where in the html the data we need is and store it.\n",
    "\n",
    "- Open the page in a web browser and analyse it.\n",
    "- Figure out where the important data is.\n",
    "- Store this data in a dictionary or a database.\n",
    "\n",
    "### Working with Data Frames\n",
    "\n",
    "Data Frames are usefull to convert your data to a file better suited for data analisys.\n",
    "\n",
    "- We will use Pandas\n",
    "- Create a dataframe from our dict and export it to a file.\n",
    "\n",
    "## Storing images\n",
    "\n",
    "Sometimes we are interested in storing images as well. To do this we use urlretrieve.\n",
    "\n",
    "- Find the source (src) of the image.\n",
    "- Use urlretrieve to download it to a directory.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from urllib.request import Request, urlopen, urlretrieve\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def clean_input(input):\n",
    "    return \" \".join(input.split()).replace('> <', '><')\n",
    "\n",
    "url = 'https://alura-site-scraping.herokuapp.com/index.php'\n",
    "headers = {'User-Agent': 'Chrome/76.0.3809.100'} # Creating a headers dictionary.\n",
    "\n",
    "req = Request(url, headers = headers) # Creating the request using the headers dictionary.\n",
    "response = urlopen(req)\n",
    "b = response.read()\n",
    "\n",
    "html = clean_input(b.decode('utf-8')) # Decoding and cleaning.\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser') # Parsing.\n",
    "\n",
    "cards = [] # Array to hold all cards.\n",
    "card = {} # Dictionary to store the information.\n",
    "\n",
    "anuncio = soup.find('div', {'class': 'well card'}) # Find one add in the page.\n",
    "\n",
    "infos = anuncio.find('div', {'class':'body-card'}).find_all('p') # Find the infos inside a body of card.\n",
    "\n",
    "for info in infos:\n",
    "    card[info.get('class')[0].split('-')[-1]] = info.get_text() # Storing infos to dict dynamicaly.\n",
    "\n",
    "card['valor'] = anuncio.find('p',{'class':'txt-value'}).get_text() # Storing more info from other divs\n",
    "\n",
    "items = anuncio.find('div', {'class':'body-card'}).ul.find_all('li') # Find the accessories inside a body of card.\n",
    "\n",
    "items.pop() # Removing last item '...'\n",
    "\n",
    "accessories = [] # Create array for accessories\n",
    "\n",
    "for a in items :\n",
    "        accessories.append(a.get_text().replace('â–º ', '')) # Adding acessories in array\n",
    "\n",
    "card['accessories'] = accessories # Adding to dict\n",
    "\n",
    "dataset = pd.DataFrame.from_dict(card, orient = 'index').T # Creating a data Frame from our card data.\n",
    "dataset.to_csv('./output/data/dataset.csv', sep=';', index = False, encoding = 'utf-8-sig') # exporting the info.\n",
    "\n",
    "image = anuncio.find('div', {'class':'image-card'}).find('img') # Find the image inside the card.\n",
    "urlretrieve(image.get('src'), './output/img/' + image.get('src').split('/')[-1]) # Downloading the image to a data folder"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}